\chapter{Related Work}

Before we discuss the design of \emph{Colonizers} and its implmentation,
let us first make an overview of existing related work. This chapter will
demonstrate why existing frameworks would not be a~good fit for \emph{Colonizers}.
We are interested mainly in two areas here:
\begin{itemize}
    \item Implementation of similar games, and frameworks facilitating that
    \item Algorithms adapted for multi-player games and algorithms adapted
        for imperfect information games
\end{itemize}

\section{Game Frameworks}

\subsection{OpenAI Gym}

OpenAI Gym is "a toolkit for developing and comparing reinforcement learning algorithms"
\cite{Openaigym}. It is a popular tool in the reinforcement learning field,
because it is modular, and easy to work with. It features a standardized API for
all of its environments (games or problems). This means that agents built for one
environment can be easily transitioned to other environments, without having to
structurally rebuild it. Another benefit is~the fact that it~is easy to create
new environments, and these newly created environments can be used by anyone,
since the API is standardized.

\autoref{figrw:gym} is an example (as presented in the OpenAI Gym documentation
\cite{Openaigym}) of a Python program which solves one of the simpler environments
available out-of-the-box in OpenAI Gym.

\begin{figure}[h!]
\begin{code}
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20):
    observation = env.reset()
    for t in range(100):
        env.render()
        print(observation)
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)
        if done:
            print("Episode finished after {} timesteps".format(t+1))
            break
env.close()
\end{code}
\caption{OpenAI Gym --- AI implementaion.}\label{figrw:gym}
\end{figure}

The~environment being solved (\emph{CartPole-v0}) is a~task where the~AI must balance a~pole
by moving the~cart below it left and right. The agent only performs random moves,
but the~example clearly illustrates how the~agent interacts with the~environment.

OpenAI Gym is not particularly suitable for the study of multi-player games with imperfect
information for a~few reasons:
\begin{itemize}
    \item It only supports reinforcement learning agents. The API is designed with this
        in mind, and does not provide support for any other machine learning methods.
    \item It does not provide any tools for determinization
        \footnote{By the determinization
        of a game state, we understand the conversion of a game state with hidden
        information into a game state with perfect information. Determinization
        takes into account the information set of the given player. For example, we can
        imagine a~poker player who has been dealt a hand which includes the Queen
        of Hearts. When this player is thinking about what other players may have,
        the Queen of Hearts is out of the question, since the player has it, and there
        is only one in the deck. Therefore, a rational determinization of a poker
        game state would be to take all cards which started in the deck, remove
        the ones the player is holding, and then randomly assign other cards to the
        other players.} 
        of imperfect information states. This would force AIs
        to track their own information sets, and to then produce determinizations
        of game states on their own.
\end{itemize}

In spite of that, there is something we can take away from OpenAI Gym when designing
\emph{Colonizers}. Notably, the API is very elegant, and creating an~AI which simply
plays random moves is a matter of very few lines of code. We will try to achieve
this with \emph{Colonizers}.

\subsection{boardgame.io}

boardgame.io \cite{Boardgameio} is a game engine for creating turn-based games.
It features many helpful features for creating board games, such as support for
multiplayer, randomness, imperfect information, and a few other useful features.

Using boardgame.io for the implementation of \emph{Colonizers} would make many things
much simpler, notably the implementation of game logic would be trivial.
However, it is also not suitable for the purposes of this thesis, because the AI
support is poor. The engine does feature a degree of AI support, but the API
is limited to using pre-existing AIs which ship with the game. The AIs which
ship with the game are an~MCTS (Monte Carlo Tree Search) AI and
a~random AI. The API for AI players only provides a~method for us to list the legal moves in
a~given game state --- it does not however provide ways to implement a~fully
custom AI.

\clearpage
\section{Algorithms}

Here we will discuss several existing algorithms which are applicable to \emph{Colonizers}.
This includes algorithms which will need to be adapted in order to be useful in our
situation, and algorithms which will work mostly out-of-the-box.

\subsection{MaxN}

Most work in the field of game-playing algorithms has traditionally been done
in games which involve two players, perfect information, finite games which
do not feature random processes. These games are also often constant-sum, therefore
they cannot feature cooperative strategies. One of the most well-known
algorithms from this field is the Minimax algorithm. The pseudocode
in \autoref{figrw:minimax} demonstrates the Minimax algorithm.

\begin{figure}[h!]
\begin{code}
def minimax(node, depth, isMaximizing):
    if depth == 0 or node is terminal:
        return node.heuristicValue
    if isMaximizing:
        value = -inf
        for child in node.children:
            value = max(value, minimax(child, depth - 1, False))
        return value
    else:
        value = +inf
        for child in node.children:
            value = min(value, minimax(child, depth - 1, True))
        return value
\end{code}
\caption{Minimax algorithm.}\label{figrw:minimax}
\end{figure}

Since Minimax is only useful in the aforementioned types of games, we will look
to the MaxN algorithm \cite{Luckhardt86}.

The MaxN algorithm is not an extension of Minimax strictly speaking, but it does
apply the driving principles of Minimax to games with more than two players.
To introduce multiple players and a non-constant sum game to Minimax, MaxN
changes the way the game is viewed. Rather than the other players trying to minimize
the player's gain, each player is trying to maximize their own gain independently.
Each game state has an associated payoff vector, where the i-th position of the vector
contains the payoff for player i in this state.

\clearpage
The procedure MaxN is defined recursively (as presented by Luckhardt and Irani
\cite{Luckhardt86}) in \autoref{figrw:maxn}.

\begin{figure}[h!]
\begin{code}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8}]
(1) For a terminal node,
    maxn(node) = payoff vector for node
(2) Given node is a move for player i, and
    $(v_{1j},...,v_{nj})$ is maxn($j^{th}$ child of node), then
    maxn(node) = $(v_{1}^{*},...,v_{n}^{*})$,
    which is the vector where $v_{i}^{*} = \max\limits_{j} v_{ij}$.
\end{code}
\caption{MaxN algorithm.}\label{figrw:maxn}
\end{figure}

We can see an example of MaxN evaluating a state tree in a three-player
game in \autoref{figrw:maxntree}. Observe how at each level, the player
on turn chooses the action which gives them the highest reward.

\begin{figure}[h!]
\Tree[.A(1,1,1) [.B(0,4,4)  [.C(2,2,5)  [.(3,3,3) ]
                                        [.(2,2,5) ]]
                            [.C(0,4,4)  [.(2,5,2) ]
                                        [.(0,4,4) ]]]
                [.B(1,1,1)  [.C(4,0,4)  [.(5,2,2) ]
                                        [.(4,0,4) ]]
                            [.C(1,1,1)  [.(4,4,0) ]
                                        [.(1,1,1) ]]]]
\caption{MaxN tree example.}\label{figrw:maxntree}
\end{figure}

The MaxN algorithm itself does not solve the problem of imperfect information.
Therefore we will describe the extension of MaxN to imperfect information
games in \autoref{sec:algomaxn}.

\clearpage
\subsection{Monte Carlo Tree Search}
\label{rw:mcts}

Monte Carlo Tree Search \cite{Chaslot10} is an algorithm for searching trees.
When we talk about MCTS in the context of game playing algorithms, MCTS
consists of two parts: a moderately shallow tree, and deep simulated games.
The algorithm grows its tree structure by adding one node at a time, and then
performing a game simulation from the position associated with the node.
The reward gained from the result of the game playout is then backpropagated
up the tree. After iterating, MCTS can then choose the best move in the root
node by simply choosing the node with the best accumulated reward.

While MCTS is applicable to games with perfect information, it needs to be adapted
for games with imperfect information. A popular approach is determinization,
which converts states with imperfect information to states with perfect information
by sampling information sets (an information set is a set of states
which are possible with respect to the information available to the player)
\cite{Cowling12}.

While determinization is a viable strategy, it is not without its pitfalls.
Russel and Norvig speculate that since all information is revealed after
determinization, the resulting AI will never make information-gathering
plays. \cite{Russell09}

Another potential issue is the fact that determinization does not take into account
the fact that opponents have a degree of uncertainty about the player's own hidden information.
Whitehouse, Powley and Cowling point out that "Determinization does not randomise the
player's own cards, and information set trees are built solely
from the point of view of the root player. In a sense this is a
worst case assumption, but it does mean that these
algorithms can never exploit the opponents' lack of
information." \cite{Whitehouse11}

Other potential problems include two mentioned by Frank and Basin \cite{Frank98}.
The first is \emph{strategy fusion}. This occurs whenever an algorithm attempts to combine
strategies from particular worlds to produce an optimal strategy for all worlds.
Quoting Frank and Basin: "The flaw in this approach
occurs because of the property of incomplete information games that the exact state
of the world at any given point of play may not be known to a player. This imposes
a constraint on a player's strategy that he must behave the same way in all possible
worlds at such points; a constraint typically broken when combining strategies designed
for individual worlds". The second issue they identified is \emph{non-locality},
whereby certain determinizations may be essentially irrelevant, since players have
the ability to avoid them with gameplay decisions.

Some variants of MCTS try to use determinization in clever ways to avoid its drawbacks.
We will be looking at ISMCTS (Information Set Monte Carlo Tree Search)
\cite{Cowling12}, in particular we are interested in the SO-ISMCTS variant.
In order to overcome the obstacles associated with determinization, SO-ISMCTS
tree nodes correspond to information sets rather than game states. Specifically,
they correspond to information sets from the root player's point of view. This
means that if we choose a determinization for a~SO-ISMCTS tree node, it is likely
that many of that node's edges will not be valid moves in the context
of the determinized state. Therefore, SO-ISMCTS limits the tree into the subtree
of valid moves with respect to a given determinization when descending the tree.
In order to balance exploration and exploitation of actions in the tree,
a multi-armed bandit algorithm is used during tree traversal.

\autoref{figrw:soismcts} shows high-level pseudocode for the SO-ISMCTS algorithm,
as presented by Cowling, Powley and Whitehouse \cite{Cowling12}.

\begin{figure}[h!]
\begin{code}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8}]
def SO-ISMCTS($[s_{0}]^{\sim1}, n$):
    create a single-node tree with root $v_{0}$ corresponding to the
        root information set $[s_{0}]^{\sim1}$ (from player 1's viewpoint)
    for n iterations do:
        choose a determinization $d$ at random from $[s_{0}]^{\sim1}$, and
        use only nodes/actions compatible with $d$ this iteration
        
        # Selection
        repeat
            descend the tree (restricted to nodes/acitons compatible 
            with $d$) using the chosen bandit algorithm
        until a node $v$ is reached such taht some action from $v$ leads
        to a player 1 information set which is not
        currently in the tree or until $v$ is terminal
        
        # Expansion
        if $v$ is nonterminal:
            choose at random an action $a$ from node $v$ that is
            compatible with $d$ and does not exist in the tree
            add a child node to $v$ corresponding to the player
            1 information set reached using action $a$ and set
            it as the new current node $v$

        # Simulation
        run a simulation from $v$ to the end of the game using
        determinization $d$

        # Backpropagation
        for each node $u$ visited during this iteration do
            update $u$'s visit count and total simulation reward
            for each sibling $w$ of $u$ that was available for
            selection when $u$ was selected, including $u$ itself do
                update $w$'s availability count

    return an action from the root node $v_{0}$ such that the
    number of visits to the corresponding child node is maximal
\end{code}
\caption{SO-ISMCTS algorithm.}\label{figrw:soismcts}
\end{figure}
