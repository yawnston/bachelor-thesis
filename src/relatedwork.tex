\chapter{Related Work}

Before we discuss the design of \emph{Colonizers} and its implmentation,
let us first make an overview of existing related work. This chapter will
demonstrate why existing frameworks would not be a~good fit for \emph{Colonizers}.
We are interested mainly in two areas here:
\begin{itemize}
    \item Implementation of similar games, and frameworks facilitating that
    \item Algorithms adapted for multi-player games and algorithms adapted
        for imperfect information games
\end{itemize}

\section{Game Frameworks}

\subsection{OpenAI Gym}

OpenAI Gym is "a toolkit for developing and comparing reinforcement learning algorithms"
\cite{Openaigym}. It is a popular tool in the reinforcement learning field,
because it is modular, and easy to work with. It features a standardized API for
all of its environments (games or problems). This means that agents built for one
environment can be easily transitioned to other environments, without having to
structurally rebuild it. Another benefit is~the fact that it~is easy to create
new environments, and these newly created environments can be used by anyone,
since the API is standardized.

\Cref{figrw:gym} is an example (as presented in the OpenAI Gym documentation
\cite{Openaigym}) of a Python program which solves one of the simpler environments
available out-of-the-box in OpenAI Gym.

\begin{figure}[h!]
\begin{code}
import gym
env = gym.make('CartPole-v0')
for i_episode in range(20):
    observation = env.reset()
    for t in range(100):
        env.render()
        print(observation)
        action = env.action_space.sample()
        observation, reward, done, info = env.step(action)
        if done:
            print("Episode finished after {} timesteps".format(t+1))
            break
env.close()
\end{code}
\caption{OpenAI Gym --- AI implementaion.}\label{figrw:gym}
\end{figure}

The~environment being solved (\emph{CartPole-v0}) is a~task where the~AI must balance a~pole
by moving the~cart below it left and right. The agent only performs random moves,
but the~example clearly illustrates how the~agent interacts with the~environment.

OpenAI Gym is not particularly suitable for the study of multi-player games with imperfect
information for a~few reasons:
\begin{itemize}
    \item It only supports reinforcement learning agents. The API is designed with this
        in mind, and only provides appropriate tools to machine learning approaches.
        Other kinds of artificial intelligence are not very well supported.
    \item It does not provide any tools for determinization
        \footnote{By the determinization
        of a game state, we understand the conversion of a game state with hidden
        information into a game state with perfect information. Determinization
        takes into account the information set of the given player. For example, we can
        imagine a~poker player who has been dealt a hand which includes the Queen
        of Hearts. When this player is thinking about what other players may have,
        the Queen of Hearts is out of the question, since the player has it, and there
        is only one in the deck. Therefore, a rational determinization of a poker
        game state would be to take all cards which started in the deck, remove
        the ones the player is holding, and then randomly assign other cards to the
        other players.} 
        of imperfect information states. This would force AIs
        to track their own information sets, and to then produce determinizations
        of game states on their own.
\end{itemize}

In spite of that, there is something we can take away from OpenAI Gym when designing
\emph{Colonizers}. Notably, the API is very elegant, and creating an~AI which simply
plays random moves is a matter of very few lines of code. We will try to achieve
this with \emph{Colonizers}.

\subsection{boardgame.io}

boardgame.io \cite{Boardgameio} is a game engine for creating turn-based games.
It features many helpful features for creating board games, such as support for
multiplayer, randomness, imperfect information, and a few other useful features.

Using boardgame.io for the implementation of \emph{Colonizers} would make many things
much simpler, notably the implementation of game logic would be trivial.
However, it is also not suitable for the purposes of this thesis, because the AI
support is poor. The engine does feature a degree of AI support, but the API
is limited to using pre-existing AIs which ship with the game. The AIs which
ship with the game are an~MCTS (Monte Carlo Tree Search) AI and
a~random AI. The API for AI players only provides a~method for us to list the legal moves in
a~given game state --- it does not however provide ways to implement a~fully
custom AI.

\clearpage
\section{Algorithms}

Here we will discuss several existing algorithms which are applicable to \emph{Colonizers}.
This includes algorithms which will need to be adapted in order to be useful in our
situation, and algorithms which will work mostly out-of-the-box.

\subsection{MaxN}
\label{rw:maxn}

Most work in the field of game-playing algorithms has traditionally been done
in games which involve two players, perfect information, finite games which
do not feature random processes. These games are also often constant-sum, therefore
they cannot feature cooperative strategies. One of the most well-known
algorithms from this field is the Minimax algorithm \cite{Millington09}. The pseudocode
in \Cref{figrw:minimax} demonstrates the Minimax algorithm.

\begin{figure}[h!]
\begin{code}
def minimax(node, depth, isMaximizing):
    if depth == 0 or node is terminal:
        return node.heuristicValue
    if isMaximizing:
        value = -inf
        for child in node.children:
            value = max(value, minimax(child, depth - 1, False))
        return value
    else:
        value = +inf
        for child in node.children:
            value = min(value, minimax(child, depth - 1, True))
        return value
\end{code}
\caption{Minimax algorithm \cite{Millington09}.}\label{figrw:minimax}
\end{figure}

The core principle of minimax is the fact that in a two-player zero-sum game,
one player's gain is the other player's loss. Therefore from the point of view
of the opponent, minimizing the player's score also means maximizing their own.
When evaluating the game tree, both players can use the same metric to make decisions
--- one player is maximizing it, and the other is minimizing it.
The nodes of the tree alternate every level --- in the root state, the current player
is maximizing the value of nodes, and in the children of the root state, the other
player is minimizing the value of nodes. In other words, if the root state is associated
with player A, player A will attempt to choose a~node with a~maximal value at every
even level of the tree (assuming the root node is level 0). Player B will then attempt
to choose the least valuable child in nodes at odd levels in the tree.
Minimax explores the tree of game states up to a depth $d$, whereby rather than
deepening the tree, it uses a positional evaluation function to evaluate the leaf nodes.
As found by Hoki and Kaneko \cite{Hoki14}, the quality of the positional evaluation
function is very critical to the performance of Minimax.

If we want to apply a Minimax-like method to \emph{Colonizers}, we must move away
somewhat from the original Minimax algorithm. In multi-player games, the game
is often not a zero-sum game. Minimax can still be applied in this situation, though
we have to make a relatively strong assumption that the goal of other players
is to minimize the player's score. We often cannot afford to make this assumption,
since this would mean that opponents have no regard for their own points.
Since Minimax is not powerful enough for our use case, we will look
to the MaxN algorithm \cite{Luckhardt86}.

The MaxN algorithm is not an extension of Minimax strictly speaking, but it does
apply the driving principles of Minimax to games with more than two players.
To introduce multiple players and a non-constant sum game to Minimax, MaxN
changes the way the game is viewed. Rather than the other players trying to minimize
the player's gain, each player is trying to maximize their own gain independently.
Each game state has an associated payoff vector, where the $i$-th position of the vector
contains the payoff for player $i$ in this state. Just like in Minimax, levels
in the tree correspond to players making decisions on their turns. In the context
of MaxN, this means the $i$-th player maximizing the $i$-th position of the reward vector
among the current node's children.

The procedure MaxN is defined recursively (as presented by Luckhardt and Irani
\cite{Luckhardt86}) in \Cref{figrw:maxn}.

\begin{figure}[h!]
\begin{code}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8}]
(1) For a terminal node,
    maxn(node) = payoff vector for node
(2) Given node is a move for player i, and
    $(v_{1j},...,v_{nj})$ is maxn($j^{th}$ child of node), then
    maxn(node) = $(v_{1}^{*},...,v_{n}^{*})$,
    which is the vector where $v_{i}^{*} = \max\limits_{j} v_{ij}$.
\end{code}
\caption{MaxN algorithm \cite{Luckhardt86}.}\label{figrw:maxn}
\end{figure}

It is important to mention that MaxN still uses a positional evaluation function,
much like Minimax. However, it is not enough to only evaluate leaf nodes
once. Since each node must have a value for every player, we must run
the evaluation function once from the perspective of each player in the game.
Notice that even though each player is only maximizing their own value
and disregarding that of others, this does not necessarily extend to the
positional evaluation function itself. The evaluation function could
reasonably take into account the standing of other players and boost
the evaluation if other players are doing poorly.

We can see an example of MaxN evaluating a state tree in a three-player
game in \Cref{figrw:maxntree}. Observe how at each level, the player
on turn chooses the action which gives them the highest reward.

\begin{figure}[ht]
\Tree[.A(1,1,1) [.B(0,4,4)  [.C(2,2,5)  [.(3,3,3) ]
                                        [.(2,2,5) ]]
                            [.C(0,4,4)  [.(2,5,2) ]
                                        [.(0,4,4) ]]]
                [.B(1,1,1)  [.C(4,0,4)  [.(5,2,2) ]
                                        [.(4,0,4) ]]
                            [.C(1,1,1)  [.(4,4,0) ]
                                        [.(1,1,1) ]]]]
\caption{MaxN tree example \cite{Luckhardt86}.}\label{figrw:maxntree}
\end{figure}

The MaxN algorithm itself does not solve the problem of imperfect information.
Therefore we will describe the extension of MaxN to imperfect information
games in \Cref{sec:algomaxn}.

\subsection{Monte Carlo Tree Search}
\label{rw:mcts}

Monte Carlo Tree Search \cite{Chaslot10} is a class of algorithms for searching trees.
In comparison with the Minimax class of algorithms, it does not feature
a positional evaluation function. Instead it uses simulation to evaluate positions.
When we talk about MCTS in the context of game playing algorithms, MCTS
consists of two parts: a moderately shallow tree, and deep simulated games.
The algorithm grows its tree structure by adding one node at a time, and then
performing a game simulation from the position associated with the node.
The reward gained from the result of the game playout is then backpropagated
up the tree. After iterating, MCTS can then choose the best move in the root
node by simply choosing the node with the best accumulated reward.

We can divide a MCTS-class algorithm into four main stages:
\begin{itemize}
    \item \emph{Selection}: Descend the state tree until a node is reached
        such that at least one of the actions that leads from it has not
        yet been explored. We will speak more about the consideration which
        goes into descending the tree later in this section.
    \item \emph{Expansion}: Choose an action that has not yet been explored
        and add its corresponding node into the tree.
    \item \emph{Simulation}: Run a simulation of the game from the newly
        added state.
    \item \emph{Backpropagation}: Propagate the reward from the result
        of the simulation back through the way we descended the tree.
\end{itemize}

A concept important to MCTS-class algorithms is the multi-armed bandit problem.
We can best understand the problem by reasoning about the following example.
Suppose that we have a slot machine with two arms, and both arms have properties
which are not known to us. We start by pulling the left arm, and get a payout
of 1. In this situation, it would seem reasonable to try the right arm at least once,
in case its payouts were much higher than those of the left arm. Let us then assume that
we pull the right arm and get a payout of $0.5$. Should we keep pulling the left arm
or the right arm? The answer to this is not easy, and there are multiple types
of algorithms designed to solve the problem of exploration versus exploitation,
including stochastic and adaptive methods \cite{Slivkins19}.

We can observe that the problem of choosing whether to perform playouts in a~node
where we won many times previously, or whether to try new nodes is a~kind of
multi-armed problem. We can therefore apply algorithms designed to solve
multi-armed problems in choosing which nodes to explore and which nodes
to exploit in MCTS-class algorithms.

While MCTS is applicable to games with perfect information, it needs to be adapted
for games with imperfect information. A popular approach is determinization,
which converts states with imperfect information to states with perfect information
by sampling information sets (an information set is a set of states
which are possible with respect to the information available to the player)
\cite{Cowling12}.

While determinization is a viable strategy, it is not without its pitfalls.
Russel and Norvig \cite{Russell09} speculate that since all information is revealed after
determinization, the resulting AI will never make information-gathering
plays.

Another potential issue is the fact that determinization does not take into account
the fact that opponents have a degree of uncertainty about the player's own hidden information.
Whitehouse, Powley and Cowling \cite{Whitehouse11} point out that "Determinization
does not randomise the player's own cards, and information set trees are built solely
from the point of view of the root player. In a sense this is a
worst case assumption, but it does mean that these
algorithms can never exploit the opponents' lack of
information".

Other potential problems include two mentioned by Frank and Basin \cite{Frank98}.
The first is \emph{strategy fusion}. This occurs whenever an algorithm attempts to combine
strategies from particular worlds to produce an optimal strategy for all worlds.
Quoting Frank and Basin \cite{Frank98}: "\emph{The flaw in this approach
occurs because of the property of incomplete information games that the exact state
of the world at any given point of play may not be known to a~player. This imposes
a~constraint on a~player's strategy that he must behave the same way in all possible
worlds at such points; a~constraint typically broken when combining strategies designed
for individual worlds}". The second issue they identified is \emph{non-locality},
whereby certain determinizations may be essentially irrelevant, since players have
the ability to avoid them with gameplay decisions.

Some variants of MCTS try to use determinization in clever ways to avoid its drawbacks.
We will be looking at ISMCTS (Information Set Monte Carlo Tree Search)
\cite{Cowling12}, in particular we are interested in the SO-ISMCTS variant.
In order to overcome the obstacles associated with determinization, SO-ISMCTS
tree nodes correspond to information sets rather than game states. Specifically,
they correspond to information sets from the root player's point of view. This
means that if we choose a determinization for a~SO-ISMCTS tree node, it is likely
that many of that node's edges will not be valid moves in the context
of the determinized state. Therefore, SO-ISMCTS limits the tree into the subtree
of valid moves with respect to a given determinization when descending the tree.

Cowling, Powley and Whitehouse \cite{Cowling12} also present two other versions
of ISMCTS, namely SO-ISMCTS + POM and MO-ISMCTS. We discuss their merits
in \Cref{sec:algoismcts}.


\Cref{figrw:soismcts} shows high-level pseudocode for the SO-ISMCTS algorithm,
as presented by Cowling, Powley and Whitehouse \cite{Cowling12}.

\begin{figure}[ht]
\begin{code}[commandchars=\\\{\},codes={\catcode`\$=3\catcode`\^=7\catcode`\_=8}]
def SO-ISMCTS($[s_{0}]^{\sim1}, n$):
    create a single-node tree with root $v_{0}$ corresponding to the
        root information set $[s_{0}]^{\sim1}$ (from player 1's viewpoint)
    for n iterations do:
        choose a determinization $d$ at random from $[s_{0}]^{\sim1}$, and
        use only nodes/actions compatible with $d$ this iteration
        
        # Selection
        repeat
            descend the tree (restricted to nodes/acitons compatible 
            with $d$) using the chosen bandit algorithm
        until a node $v$ is reached such that some action from $v$ leads
        to a player 1 information set which is not
        currently in the tree or until $v$ is terminal
        
        # Expansion
        if $v$ is nonterminal:
            choose at random an action $a$ from node $v$ that is
            compatible with $d$ and does not exist in the tree
            add a child node to $v$ corresponding to the player
            1 information set reached using action $a$ and set
            it as the new current node $v$

        # Simulation
        run a simulation from $v$ to the end of the game using
        determinization $d$

        # Backpropagation
        for each node $u$ visited during this iteration do
            update $u$'s visit count and total simulation reward
            for each sibling $w$ of $u$ that was available for
            selection when $u$ was selected, including $u$ itself do
                update $w$'s availability count

    return an action from the root node $v_{0}$ such that the
    number of visits to the corresponding child node is maximal
\end{code}
\caption{SO-ISMCTS algorithm \cite{Cowling12}.}\label{figrw:soismcts}
\end{figure}
