\chapter{Experiment Description}

There are two qualities which we want to analyze with respect to~the~game
and the implemented AI algorithms:

\begin{itemize}
    \item Identify potential asymmetries in game balance
    \item Compare methodologies used by the AI algorithms
\end{itemize}

To this end, we conducted five experiments, split according to their purpose.
The following sections elaborate on the experiments and their results.

\section{Game Balance Experiments}

As mentioned in \autoref{chap:gamedesign}, the game features a degree of asymmetry.
The order in which players take their turns inherently changes the viability of certain
strategies, because players in different positions have different information sections
available to them. For example, the~player in~the~first position has perfect information
about which colonist was removed from play during the colonist pick phase, while
the~second and third players do not have such certainty.

Most importantly however, a~player's colonist is revealed at the~start of their
turn. This means that if the player in the fourth position is a~Spy or an~Opportunist,
they will know all the other players' colonists when their turn comes around.
This means that this player will be able to target any player with their targeted
ability without the fear of missing or hitting an unintended target.

With these things in mind, we can hypothesize that players in the earlier positions
have an easier time achieving synergy-based strategies, since they get priority
when picking colonists. On the other hand, we can also hypothesize that players in~later
positions will benefit from play based around using targeted colonist abilities.

In Chess, it is widely agreed that the white player has an~advantage \cite{Streeter46}. 
Similarly, we aim to~discover whether player ordering confers
a~measurable advantage to any player in \emph{Colonizers}.
We will conduct this experiment with the null hypothesis --- we assume
that there is no significant advantage for any player ordering.
\pagebreak

\subsection{Description}

We conducted two experiments in this section. In both of them, four identical AIs
played 1000 games against each other. In the first experiment, the AI in question was
\texttt{RandomIntelligence}, and in the second
experiment it was \texttt{HeuristicIntelligence}.

All random events were seeded, and the results of the games were captured in JSON
(JavaScript Object Notation) files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \autoref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows.
Note that the chosen seeds do not have any special meaning.
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

\subsection{Findings}

\subsubsection{Experiment 1}

First off, let us focus on the experiment runs with \texttt{RandomIntelligence}.
Results of the 1000 runs can seen in table \ref{tabex:randomwins}.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{Position} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\
\midrule
Wins            & 310 & 213   & 251   & 226 \\
Losses          & 197 & 261   & 279   & 263 \\
Average rank    & 2.3 & 2.572 & 2.553 & 2.575 \\
\bottomrule
\end{tabular}
\caption{Results of \texttt{RandomIntelligence} play.}\label{tabex:randomwins}
\end{table}

The most notable result we have is the fact that AIs in the first position
seem to be winning the most often. AIs in the first position also lose (place fourth)
less, and they have a better average ranking overall.

We can try to verify the significance of these results mathematically.
If we assume that the rank at the end of the game follows a normal distribution,
we can compute a confidence interval. Let $\bar x$ be the sample mean, let
$s$ be the sample standard deviation and let $n$ be the sample size.
We are looking for a confidence interval for the unknown mean $\mu$.
The~$100(1-\alpha)\%$ confidence interval for $\mu$ is
$$\hat \mu_L =\bar x - z_{\alpha/2} \cdot s/\sqrt{n},\;
\hat \mu_U=\bar x + z_{\alpha/2} \cdot s/\sqrt{n},$$

The sample mean for rank among first position AIs is $2.3$ as seen in
table \ref{tabex:randomwins}, and the sample standard deviation is
$1.1069$. If we want a $95\%$ confidence interval, we will use
$z_{\alpha/2} = 1.96$. This gives us the confidence interval of
$$\hat \mu_L = 2.2314,\;\hat \mu_U = 2.3686$$

We can also compute a $95\%$ confidence interval for the mean rank of AIs in position 3:
$$\hat \mu_L = 2.4821,\;\hat \mu_U = 2.6239$$

These intervals do not overlap, therefore we can conclude that there is a statistically
significant difference between the rank means among AIs at different positions. This may
indicate a potential balance issue in the rules of the game, with the first position
being more powerful than the other ones, which are similar in power. However,
measurement on randomly choosing AIs does not necessarily indicate imbalance, since
random agents do not play optimal strategies. Therefore we cannot conclude anything
about game balance just yet, but this statistical difference is worth keeping in mind.

\subsubsection{Experiment 2}

The other experiment in this section is very similar to the first one, except
we have four instances of \texttt{HeuristicIntelligence} instead of
four instances of \texttt{RandomIntelligence}
playing against each other. Results of the 1000 runs can seen
in table \ref{tabex:heuristicwins}.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{Position} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\
\midrule
Wins            & 230 & 202   & 282   & 286 \\
Losses          & 415 & 298   & 152   & 135 \\
Average rank    & 2.8 & 2.67 & 2.302 & 2.228 \\
\bottomrule
\end{tabular}
\caption{Results of \texttt{HeuristicIntelligence} play.}\label{tabex:heuristicwins}
\end{table}

If we compare these results to those in table \ref{tabex:randomwins}, we can see
almost exactly the opposite results. With random AIs playing, we saw that the AI
in the first position had a statistically significant advantage. With heuristically
driven AIs, it is obvious on first glance that earlier positions are less powerful
and later positions are more powerful. We can verify this statistically by computing
confidence intervals for the first and fourth ranks. The $99\%$ confidence interval
for the first position is
$$\hat \mu_L = 2.7019,\;\hat \mu_U = 2.8981$$
while the $99\%$ confidence interval for the fourth position is
$$\hat \mu_L = 2.1458,\;\hat \mu_U = 2.3102$$
The intervals do not overlap, therefore we can conclude that there is a statistically
significant difference between the means of these positions' respective average ranks.

While the differences between wins per position are notable, the most interesting
are the loss statistics. It would appear that the earlier ranks (particularly the first one)
are susceptible to being targeted by players in other ranks. Since every player's colonist
is revealed at the start of their turn, this makes the first position an easy target
for all other players. The game does have counter-balances for this situation ---
notably the fact that the first player to build their colony to full gets four extra
victory points. However, it would seem that the heuristic AI does not have the necessary
tools to deal with being targeted down by others. This could possibly be due to
an~implementation bias inherent in the chosen heuristics, but it could also signal
a~game balance issue.

\section{Algorithm Comparison Experiments}

We have implemented four algorithms in this thesis --- \texttt{RandomIntelligence},
\texttt{HeuristicIntelligence}, \texttt{MaxnIntelligence} and \texttt{ISMCTSIntelligence}.
In order to determine the qualities of said algorithms, we will analyze their differences,
along with their advantages and disadvantages. We will also look at how the algorithms
perform in play against each other, with the hopes of determining which algorithm
is the most suitable for a game like \emph{Colonizers}.

To start with, we would not expect \texttt{RandomIntelligence} to perform well in any
kind of mutual play. It is present simply as a benchmark for the performance of
other AIs.

The~more important benchmark is \texttt{HeuristicIntelligence}, since it represents
rules which were created by observing humans play the game
\footnote{The rules were designed by the author after the author played the game
several times with his friends, who had not played a similar game before}.
Therefore we consider this
AI to be a minimum benchmark for other AIs to be competent.

\texttt{MaxnIntelligence} is based on the MaxN algorithm \cite{Luckhardt86}, which
is itself based on Minimax \cite{Millington09}. This AI was adapted for imperfect
information games, and it spends a~non-trivial amount of computing power
on simply exploring possible determinizations of the current game state.
If we take that into consideration, along with the fact that the branching factor
for \emph{Colonizers} is non-trivial, we would expect \texttt{MaxnIntelligence}
to perform relatively poorly. The depth of the search trees used could not be
reasonably increased beyond 7, due to performance concerns. We expect that any kind of
long-term strategy could not be achieved by it since it lacks the necessary
exploration depth. The Minimax family of algorithms does however offer very
solid insight into the few turns it examines, therefore we hypothesize that this AI will be
primarily good at tactics-based play. The performance of \texttt{MaxnIntelligence}
against \texttt{HeuristicIntelligence} is uncertain, therefore we will follow the null
hypothesis and assume that their performances are statistically similar.
We also hypothesize that since this AI has a strong foundation for tactical prowess,
it should win more often when in later positions (namely third and fourth).

The final AI tested is \texttt{ISMCTSIntelligence}. This AI is well-adapted to
imperfect information and multiple player environments. Therefore we would expect
it to outperform the three aforementioned AIs in most situations. We expect it
to play well in most circumstances, regardless of player permutation.

\subsection{Description}

We conducted three experiments in this section. In the first experiment, one of each
implemented intelligence played 50 games against each other. This experiment is meant
to assess the general playing ability of the AIs. In the second experiment, we
let two instances of \texttt{HeuristicIntelligence} and two instances of
\texttt{MaxnIntelligence} play against each other for 50 games.
Lastly in the third experiment, we performed the same thing as in the second experiment,
but we replaced \texttt{MaxnIntelligence} instances with \texttt{ISMCTSIntelligence}
instances. These two experiments are meant to benchmark the adapted algorithms
against the heuristic solution.

All random events were seeded, and the results of the games were captured in JSON
files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \autoref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows.
Note that the chosen seeds do not have any special meaning.
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{MaxnIntelligence}: seed 99
    \item \texttt{ISMCTSIntelligence}: seed 15
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

\subsection{Findings}

WORK IN PROGRESS

\subsubsection{Experiment 3}

\subsubsection{Experiment 4}

\subsubsection{Experiment 5}
