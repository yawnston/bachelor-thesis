\chapter{Experiment Description}

There are two qualities which we want to analyze with respect to~the~game
and the implemented AI algorithms:

\begin{itemize}
    \item Identify potential asymmetries in game balance
    \item Compare methodologies used by the AI algorithms
\end{itemize}

To this end, we conducted five experiments, split according to their purpose.
The following sections elaborate on the experiments and their results.

\section{Game Balance Experiments}

As mentioned in \autoref{chap:gamedesign}, the game features a degree of asymmetry.
The order in which players take their turns inherently changes the viability of certain
strategies, because players in different positions have different information sections
available to them. For example, the~player in~the~first position has perfect information
about which colonist was removed from play during the colonist pick phase, while
the~second and third players do not have such certainty.

Most importantly however, a~player's colonist is revealed at the~start of their
turn. This means that if the player in the fourth position is a~Spy or an~Opportunist,
they will know all the other players' colonists when their turn comes around.
This means that this player will be able to target any player with their targeted
ability without the fear of missing or hitting an unintended target.

With these things in mind, we can hypothesize that players in the earlier positions
have an easier time achieving synergy-based strategies, since they get priority
when picking colonists. On the other hand, we can also hypothesize that players in~later
positions will benefit from play based around using targeted colonist abilities.

In Chess, it is widely agreed that the white player has an~advantage \cite{Streeter46}. 
Similarly, we aim to~discover whether player ordering confers
a~measurable advantage to any player in \emph{Colonizers}.
We will conduct this experiment with the null hypothesis --- we assume
that there is no significant advantage for any player ordering.
\pagebreak

\subsection{Description}

We conducted two experiments in this section. In both of them, four identical AIs
played 1000 games against each other. In the first experiment, the AI in question was
\texttt{RandomIntelligence}, and in the second
experiment it was \texttt{HeuristicIntelligence}.

All random events were seeded, and the results of the games were captured in JSON
(JavaScript Object Notation) files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \autoref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows:
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

\subsection{Findings}

TODO

\section{Algorithm Comparison Experiments}

We have implemented four algorithms in this thesis --- \texttt{RandomIntelligence},
\texttt{HeuristicIntelligence}, \texttt{MaxnIntelligence} and \texttt{ISMCTSIntelligence}.
In order to determine the qualities of said algorithms, we will analyze their differences,
along with their advantages and disadvantages. We will also look at how the algorithms
perform in play against each other, with the hopes of determining which algorithm
is the most suitable for a game like \emph{Colonizers}.

To start with, we would not expect \texttt{RandomIntelligence} to perform well in any
kind of mutual play. It is present simply as a benchmark for the performance of
other AIs.

The~more important benchmark is \texttt{HeuristicIntelligence}, since it represents
rules which were created by observing humans play the game. Therefore we consider this
AI to be a minimum benchmark for other AIs to be competent.

\texttt{MaxnIntelligence} is based on the MaxN algorithm \cite{Luckhardt86}, which
is itself based on Minimax \cite{Millington09}. This AI was adapted for imperfect
information games, and it spends a~non-trivial amount of computing power
on simply exploring possible determinizations of the current game state.
If we take that into consideration, along with the fact that the branching factor
for \emph{Colonizers} is non-trivial, we would expect \texttt{MaxnIntelligence}
to perform relatively poorly. The depth of the search trees used could not be
reasonably increased beyond 7, due to performance concerns. We expect that any kind of
long-term strategy could not be achieved by it since it lacks the necessary
exploration depth. The Minimax family of algorithms does however offer very
solid insight into the few turns it examines, therefore we hypothesize that this AI will be
primarily good at tactics-based play. The performance of \texttt{MaxnIntelligence}
against \texttt{HeuristicIntelligence} is uncertain, therefore we will follow the null
hypothesis and assume that their performances are statistically similar.
We also hypothesize that since this AI has a strong foundation for tactical prowess,
it should win more often when in later positions (namely third and fourth).

The final AI tested is \texttt{ISMCTSIntelligence}. This AI is well-adapted to
imperfect information and multiple player environments. Therefore we would expect
it to outperform the three aforementioned AIs in most situations. We expect it
to play well in most circumstances, regardless of player permutation.

\subsection{Description}

We conducted three experiments in this section. In the first experiment, one of each
implemented intelligence played 50 games against each other. This experiment is meant
to assess the general playing ability of the AIs. In the second experiment, we
let two instances of \texttt{HeuristicIntelligence} and two instances of
\texttt{MaxnIntelligence} play against each other for 50 games.
Lastly in the third experiment, we performed the same thing as in the second experiment,
but we replaced \texttt{MaxnIntelligence} instances with \texttt{ISMCTSIntelligence}
instances. These two experiments are meant to benchmark the adapted algorithms
against the heuristic solution.

All random events were seeded, and the results of the games were captured in JSON
files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \autoref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows:
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{MaxnIntelligence}: seed 99
    \item \texttt{ISMCTSIntelligence}: seed 15
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

\subsection{Findings}

WORK IN PROGRESS
