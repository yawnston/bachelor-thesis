\chapter{Experiment Description}

There are two qualities which we want to analyze with respect to~the~game
and the implemented AI algorithms:

\begin{itemize}
    \item Identify potential asymmetries in game balance
    \item Compare methodologies used by the AI algorithms
\end{itemize}

To this end, we conducted five experiments, split according to their purpose.
The following sections elaborate on the experiments and their results.

\section{Game Balance Experiments}

As mentioned in \Cref{chap:gamedesign}, the game features a degree of asymmetry.
The order in which players take their turns inherently changes the viability of certain
strategies, because players in different positions have different information sections
available to them. For example, the~player in~the~first position has perfect information
about which colonist was removed from play during the colonist pick phase, while
the~second and third players do not have such certainty.

Most importantly however, a~player's colonist is revealed at the~start of their
turn. This means that if the player in the fourth position is a~Spy or an~Opportunist,
they will know all the other players' colonists when their turn comes around.
This means that this player will be able to target any player with their targeted
ability without the fear of missing or hitting an unintended target.

With these things in mind, we can hypothesize that players in the earlier positions
have an easier time achieving synergy-based strategies, since they get priority
when picking colonists. On the other hand, we can also hypothesize that players in~later
positions will benefit from play based around using targeted colonist abilities.

In Chess, it is widely agreed that the white player has an~advantage \cite{Streeter46}. 
Similarly, we aim to~discover whether player ordering confers
a~measurable advantage to any player in \emph{Colonizers}.
We will conduct this experiment with the null hypothesis --- we assume
that there is no significant advantage for any player ordering.
\pagebreak

\subsection{Description}

We conducted two experiments in this section. In both of them, four identical AIs
played 1000 games against each other. In the first experiment, the AI in question was
\texttt{RandomIntelligence}, and in the second
experiment it was \texttt{HeuristicIntelligence}.

All random events were seeded, and the results of the games were captured in JSON
(JavaScript Object Notation) files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \Cref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows.
Note that the chosen seeds do not have any special meaning, and they were selected
at random by the author. There is no particular reason for the algorithms to have
different seeds.
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

The algorithms' positions were shuffled at the beginning of each game with
the Fisher-Yates Shuffle \cite{Knuth98}, using the game engine's random
number generator. In this experiment this is not necessary since we have
four instances of the same algorithm. This experiment was performed
using the same method as other experiments where the shuffling is necessary,
therefore the shuffling is present here. We mention this since in order to
exactly reproduce the experiment, the shuffle must be performed with the game engine's
random number generator.

\subsection{Findings}

\subsubsection{Experiment 1}

First off, let us focus on the experiment runs with \texttt{RandomIntelligence}.
Results of the 1000 runs can be seen in table \ref{tabex:randomwins} Note that
by "Losses" we mean fourth-place finishes, not failing to finish first.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{Position} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\
\midrule
Wins            & 310 & 213   & 251   & 226 \\
Losses          & 197 & 261   & 279   & 263 \\
Average rank    & 2.3 & 2.572 & 2.553 & 2.575 \\
\bottomrule
\end{tabular}
\caption{Results of \texttt{RandomIntelligence} play.}\label{tabex:randomwins}
\end{table}

The most notable result we have is the fact that AIs in the first position
seem to be winning the most often. AIs in the first position also lose (place fourth)
less, and they have a better average ranking overall.

We can try to verify the significance of these results mathematically.
We will employ the $\chi^{2}$ test to check whether the number of wins for
position 1 follows the binomial distribution $B(1,0.25)$. The null hypothesis
in this case is that the win rate follows the aforementioned binomial distribution.
With 1000 trials, we would expect 250 of them to succeed and 750 of them to fail.
We can compute $\chi^{2}$ as follows
$$\chi^{2} = \frac{(310 - 250)^{2}}{250} + \frac{(690 - 750)^{2}}{750} \approx 19.2$$
This gives us the distribution $\chi^{2}(1)$ with 1 degree of freedom. Our
$\chi^{2}$ test statistic is $19.2$, which gives us a $p$-value of 0.00001.
If we consider a significance level of $0.05$, we can reject the null hypothesis.

This may
indicate a potential balance issue in the rules of the game, with the first position
being more powerful than the other ones, which appear to be similar in power. However,
measurement on randomly choosing AIs does not necessarily indicate imbalance, since
random agents do not play optimal strategies. Therefore we cannot conclude anything
about game balance just yet, but this statistical difference is worth keeping in mind.

\subsubsection{Experiment 2}

The other experiment in this section is very similar to the first one, except
we have four instances of \texttt{HeuristicIntelligence} instead of
four instances of \texttt{RandomIntelligence}
playing against each other. Results of the 1000 runs can be seen
in table \ref{tabex:heuristicwins}. Note that
by "Losses" we mean fourth-place finishes, not failing to finish first.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{Position} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} \\
\midrule
Wins            & 230 & 202   & 282   & 286 \\
Losses          & 415 & 298   & 152   & 135 \\
Average rank    & 2.8 & 2.67 & 2.302 & 2.228 \\
\bottomrule
\end{tabular}
\caption{Results of \texttt{HeuristicIntelligence} play.}\label{tabex:heuristicwins}
\end{table}

If we compare these results to those in table \ref{tabex:randomwins}, we can see
almost exactly the opposite results. With random AIs playing, we saw that the AI
in the first position had a statistically significant advantage. With heuristically
driven AIs, it is obvious on first glance that earlier positions are less powerful
and later positions are more powerful. We can verify this statistically by performing
the $\chi^{2}$ test on the number of wins and losses for positions 1 and 4, similarly
to Experiment 1.

We can start with wins for position 1
$$\chi^{2} = \frac{(230 - 250)^{2}}{250} + \frac{(770 - 750)^{2}}{750} \approx 2.14$$
This gives us a~$p$-value of 0.14350. If we consider a~significance level of $0.05$,
we cannot reject the null hypothesis that the number of wins for the first position
follows the distribution $B(1,0.25)$.

If we move on to wins for position 4, we proceed as follows
$$\chi^{2} = \frac{(230 - 250)^{2}}{250} + \frac{(770 - 750)^{2}}{750} \approx 6.912$$
This gives us a $p$-value of 0.00856, therefore we can reject the null hypothesis
if we consider a~statistical significance of $0.05$.

We can also analyze the loss statistics, starting with position 1
$$\chi^{2} = \frac{(415 - 250)^{2}}{250} + \frac{(585 - 750)^{2}}{750} \approx 145.2$$
This gives us a $p$-value of nearly 0, therefore we can reject the null hypothesis
when considering a~statistical significance of $0.05$.

For losses at position 4, we have
$$\chi^{2} = \frac{(135 - 250)^{2}}{250} + \frac{(865 - 750)^{2}}{750} \approx 70.54$$
This also gives us a $p$-value of nearly 0, therefore we can again reject the null hypothesis
when considering a~statistical significance of $0.05$.

While the differences between wins per position are notable, the most interesting
are the loss statistics. It would appear that the earlier ranks (particularly the first one)
are susceptible to being targeted by players in other ranks. Since every player's colonist
is revealed at the start of their turn, this makes the first position an easy target
for all other players. The game does have counter-balances for this situation ---
notably the fact that the first player to build their colony to full gets four extra
victory points. However, it would seem that the heuristic AI does not have the necessary
tools to deal with being targeted down by others. This could possibly be due to
an~implementation bias inherent in the chosen heuristics, but it could also signal
a~game balance issue.

\section{Algorithm Comparison Experiments}

We have implemented four algorithms in this thesis --- \texttt{RandomIntelligence},
\texttt{HeuristicIntelligence}, \texttt{MaxnIntelligence} and \texttt{ISMCTSIntelligence}.
In order to determine the qualities of said algorithms, we will analyze their differences,
along with their advantages and disadvantages. We will also look at how the algorithms
perform in play against each other, with the hopes of determining which algorithm
is the most suitable for a game like \emph{Colonizers}.

To start with, we would not expect \texttt{RandomIntelligence} to perform well in any
kind of mutual play. It is present simply as a benchmark for the performance of
other AIs.

The~more important benchmark is \texttt{HeuristicIntelligence}, since it represents
rules which were created by observing humans play the game
\footnote{The rules were designed by the author after the author played the game
several times with his friends, who had not played a similar game before}.
Therefore we consider this
AI to be a minimum benchmark for other AIs to be competent.

\texttt{MaxnIntelligence} is based on the MaxN algorithm \cite{Luckhardt86}, which
is itself based on Minimax \cite{Millington09}. This AI was adapted for imperfect
information games, and it spends a~non-trivial amount of computing power
on simply exploring possible determinizations of the current game state.
If we take that into consideration, along with the fact that the branching factor
for \emph{Colonizers} is non-trivial
\footnote{This is explored in depth in \Cref{sec:branching}.}
, we would expect \texttt{MaxnIntelligence}
to perform relatively poorly. The depth of the search trees used could not be
reasonably increased beyond 7, due to performance concerns. We expect that any kind of
long-term strategy could not be achieved by it since it lacks the necessary
exploration depth. The Minimax family of algorithms does however offer very
solid insight into the few turns it examines, therefore we hypothesize that this AI will be
primarily good at tactics-based play. The performance of \texttt{MaxnIntelligence}
against \texttt{HeuristicIntelligence} is uncertain, therefore we will follow the null
hypothesis and assume that their performances are statistically similar.
We also hypothesize that since this AI has a strong foundation for tactical prowess,
it should win more often when in later positions (namely third and fourth).

The final AI tested is \texttt{ISMCTSIntelligence}. This AI is well-adapted to
imperfect information and multiple player environments. Therefore we would expect
it to outperform the three aforementioned AIs in most situations. We expect it
to play well in most circumstances, regardless of player permutation.

\subsection{Description}

We conducted three experiments in this section. In the first experiment, one of each
implemented intelligence played 50 games against each other. This experiment is meant
to assess the general playing ability of the AIs. In the second experiment, we
let two instances of \texttt{HeuristicIntelligence} and two instances of
\texttt{MaxnIntelligence} play against each other for 50 games.
Lastly in the third experiment, we performed the same thing as in the second experiment,
but we replaced \texttt{MaxnIntelligence} instances with \texttt{ISMCTSIntelligence}
instances. These two experiments are meant to benchmark the adapted algorithms
against the heuristic solution.

All random events were seeded, and the results of the games were captured in JSON
files.
The results were then parsed and analyzed. The JSON result files can be found
in~the~attached source code, refer to \Cref{chap:experimentdocs} for more information
on their location and semantics.

The random seeds used by application components during the experiment were as follows.
Note that the chosen seeds do not have any special meaning, and they were selected
at random by the author. There is no particular reason for the algorithms to have
different seeds.
\begin{itemize}
    \item \texttt{RandomIntelligence}: seed 42
    \item \texttt{HeuristicIntelligence}: seed 97
    \item \texttt{MaxnIntelligence}: seed 99
    \item \texttt{ISMCTSIntelligence}: seed 15
    \item \texttt{GameConstants}: seed was changed every game to prevent the same game from
        being played 1000 times. The seeds were generated by~a~C\# 
        random number generator seeded with 42.
\end{itemize}

The algorithms' positions were shuffled at the beginning of each game with
the Fisher-Yates Shuffle \cite{Knuth98}, using the game engine's random
number generator. We mention this since in order to
exactly reproduce the experiment, the shuffle must be performed with the game engine's
random number generator.

\subsection{Findings}

\subsubsection{Experiment 3}

In Experiment 3, we had one of each type of AI play against each other in 50 games.
Results of the 50 runs can be seen in table \ref{tabex:oneofeach} Note that
by "Losses" we mean fourth-place finishes, not failing to finish first.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Random} & \textbf{Heuristic} & \textbf{MaxN} & \textbf{ISMCTS} \\
\midrule
Wins            & 0   & 5     & 8     & 37 \\
Losses          & 40  & 1     & 9     & 0 \\
Average rank    & 3.8 & 2.38  & 2.54  & 1.28 \\
\bottomrule
\end{tabular}
\caption{Results with one of each AI.}\label{tabex:oneofeach}
\end{table}

We can immediately see that there is no point in statistically checking whether
all AIs are equally good, since the results are so extreme. ISMCTS is the clear
winner, having won the majority of games and not having lost a single one.
An interesting data point is of the 13 instances where ISMCTS didn't win, it
places second 12 times and third 1 time.
The more interesting part is the comparison of the heuristic AI with MaxN.
It would appear that the heuristic AI is much less likely to lose than MaxN.
This makes sense, considering that the heuristic AI is a collection of rules
designed to always move the AI towards gaining score, even if it is not optimal.

We can take a look at position-based data in order to gain insight into which AIs
are strong in which positions. \Cref{tabex:oneofeachpos} shows positions where
the AIs scored their wins.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Random} & \textbf{Heuristic} & \textbf{MaxN} & \textbf{ISMCTS} \\
\midrule
Position 1    & 0   & 2     & 5     & 11 \\
Position 2    & 0   & 1     & 2     & 6 \\
Position 3    & 0   & 2     & 1     & 9 \\
Position 4    & 0   & 0     & 0     & 11 \\
\bottomrule
\end{tabular}
\caption{Positions of wins in Experiment 3.}\label{tabex:oneofeachpos}
\end{table}

We can try to check whether AIs are similarly likely to win in any position.
We will not perform the statistical test on the results for the heuristic or MaxN
AIs, since the sample size for wins is too small. We can, however, perform the $\chi^{2}$
test for ISMCTS wins by position.
As the null hypothesis, we will assume that the AIs are equally likely to win in
any position. We can check the hypothesis that the win distribution for ISMCTS
between positions follows the distribution $B(1,0.25)$. ISMCTS won 37 times in total,
giving us an expected 9.25 wins per position.
$$\chi^{2} = \frac{(6 - 9.25)^{2}}{9.25} + \frac{(31 - 27.75)^{2}}{27.75} \approx 1.52$$
This gives us a $p$-value of 0.21762, therefore we cannot reject the null
hypothesis if we consider a~statistical significance of $0.05$.
This means that we cannot reject the hypothesis that ISMCTS is equally likely
to win in any position.

\subsubsection{Experiment 4}

In Experiment 4, we had two instances of \texttt{HeuristicIntelligence} and two
instances of \texttt{MaxnIntelligence} play against each other in 50 games.
Results of the 50 runs can be seen in table \ref{tabex:heurmaxn} Note that
by "Losses" we mean fourth-place finishes, not failing to finish first.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Heuristic} & \textbf{MaxN} \\
\midrule
Wins            & 35   & 15   \\
Losses          & 10   & 40   \\
Average rank    & 2.13 & 2.87 \\
\bottomrule
\end{tabular}
\caption{Results with heuristic AI versus MaxN.}\label{tabex:heurmaxn}
\end{table}

We can use the $\chi^{2}$ test to check whether there is a statistical difference
between these AIs' likelihoods to win. We will assume that the heuristic AI's
wins follow a binomial distribution $B(1,0.5)$. Then
$$\chi^{2} = \frac{(35 - 25)^{2}}{25} + \frac{(15 - 25)^{2}}{25} = 8$$
This gives us a $p$-value of 0.00468. We are considering a~statistical significance of $0.05$,
therefore we can reject the null hypothesis.
As a~consequence, we have established that in this scenario,
the heuristic AI performs significantly better than MaxN.

Since we earlier hypothesized that MaxN would be more likely to win in later
positions, we can also look at positional results as shown in \Cref{tabex:heurmaxnpos}

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Heuristic} & \textbf{MaxN} \\
\midrule
Position 1    & 13   & 13 \\
Position 2    & 15   & 2  \\
Position 3    & 4    & 0  \\
Position 4    & 2    & 0  \\
\bottomrule
\end{tabular}
\caption{Positions of wins in Experiment 4.}\label{tabex:heurmaxnpos}
\end{table}

The results here are interesting in multiple ways. The first is the fact that our earlier
hypothesis that MaxN would be strong at playing later positions was completely wrong,
with MaxN not achieving a single win in positions 3 or 4.

The other way these results are interesting is the fact that in position 1,
both AIs had the same likelihood of winning, even though MaxN is obviously statistically
inferior to its heuristic counterpart. We do see a similar trend to the results
of Experiment 3, where most wins seem to happen at earlier positions.
This could potentially indicate the existence of overpowered strategies available
only to players in early positions.

In summary, results for Experiment 4 were rather surprising, considering we rejected
both of our prior hypotheses based on the experiment results.

\subsubsection{Experiment 5}

In Experiment 5, we had two instances of \texttt{HeuristicIntelligence} and two
instances of \texttt{ISMCTSIntelligence} play against each other in 50 games.
Results of the 50 runs can be seen in table \ref{tabex:heurismcts} Note that
by "Losses" we mean fourth-place finishes, not failing to finish first.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Heuristic} & \textbf{ISMCTS} \\
\midrule
Wins            & 5    & 45   \\
Losses          & 48   & 2    \\
Average rank    & 3.26 & 1.74 \\
\bottomrule
\end{tabular}
\caption{Results with heuristic AI versus ISMCTS.}\label{tabex:heurismcts}
\end{table}

We can immediately see that there is not much point in examining these results
statistically, since it is obvious that ISMCTS is significantly better.
We will therefore analyze the positional results as shown in \Cref{tabex:heurismctspos}
in more detail, since they paint a~less obvious picture.

\begin{table}[h!]
\centering
\begin{tabular}{l@{\hspace{1.5cm}} c c c c}
\textbf{AI} & \textbf{Heuristic} & \textbf{ISMCTS} \\
\midrule
Position 1    & 2   & 9   \\
Position 2    & 2   & 13  \\
Position 3    & 0   & 14  \\
Position 4    & 1   & 9   \\
\bottomrule
\end{tabular}
\caption{Positions of wins in Experiment 5.}\label{tabex:heurismctspos}
\end{table}

We will perform a $\chi^{2}$ test to check whether the positional wins for
ISMCTS follow a~binomial distribution $B(1,0.25)$, focusing on wins in position 3
$$\chi^{2} = \frac{(14 - 11.25)^{2}}{11.25} + \frac{(31 - 33.75)^{2}}{33.75} \approx 0.896$$
This gives us a $p$-value of 0.3439. If we consider a~statistical significance of $0.05$,
we cannot reject the null hypothesis. This means that we cannot reject the hypothesis that
ISMCTS is equally likely to win in any position.

The results of Experiment 5 are consistent with the hypotheses we had prior to performing
the experiment. We confirmed that the performance of ISMCTS is much better than that
of the heuristic algorithm.
